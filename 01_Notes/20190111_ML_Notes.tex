\documentclass[12pt, authoryear]{elsarticle}
\makeatletter
\def\ps@pprintTitle{%
	\let\@oddhead\@empty
	\let\@evenhead\@empty
	\def\@oddfoot{}%
	\let\@evenfoot\@oddfoot}
\makeatother
%\usepackage{lmodern}
% My spacing
\usepackage{setspace}
\setstretch{1.5}

%\DeclareMathSizes{12}{14}{10}{10}
\usepackage[margin=2.5cm]{geometry}    % How to set margins - optimized for 2.5cm      


\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   			% ... or a4paper or a5paper or ... 
\usepackage{enumitem}
\usepackage{mathtools}
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    			% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}						% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{flafter}			
\usepackage{setspace}
%\linespread{1.5}
\usepackage[font={}]{caption}
\usepackage[bottom]{footmisc}
\usepackage[capposition=top]{floatrow}   %figure notes

%math packages 
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{graphicx,epsf,subfigure}
\usepackage{pstricks,pst-node,psfrag}
\usepackage{amsthm,amssymb,amsmath}
\usepackage{amsmath,bm}
\usepackage{amsmath}

%mathnotes
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bX}{\mbox{\boldmath $X$}}
\newcommand{\bY}{\mbox{\boldmath $Y$}}
\newcommand{\bI}{\mbox{\boldmath $I$}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\x}{\textsc{\textbf{x}}}
\newcommand{\xx}{\textsc{x}}
\newtheorem{theorem}{Theorem}

%add figure 
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{hyperref}
\usepackage[round]{natbib}

\usepackage{soul}

\def\bibsection{\section{References}} %%% Make "References" appear before bibliography
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{tablefootnote}
\usepackage{lscape} 
\usepackage{animate}
\usepackage{bbm}

\renewcommand{\contentsname}{Table of Contents} % change name from Contents to Table of Contents

\usepackage{titlesec}

\setcounter{secnumdepth}{4}

%_______________________________________________________________________________________________________%
%_______________________________________________________________________________________________________%
%\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
%\usepackage{graphicx,multirow}
\usepackage{xcolor,colortbl}
\usepackage{xcolor}
%\usepackage{graphicx,multirow}
\usepackage[capposition=top]{floatrow}
\setcounter{secnumdepth}{4}

\begin{document}

\begin{frontmatter}  %

\title{Lecture Notes Machine Learning}

\author[Add1]{Felix Adam}
\ead{felix.adam@bracelonagse.eu}

\address[Add1]{Barcelona Graduate School of Economics, Barcelona, Spain}
%\address[Add2]{Some other Institution, Cape Town, South Africa}

%\cortext[cor]{Corresponding author: Nico Katzke}

\begin{abstract}
\small{
This document contains my lecture notes for the course Machine Learning at the Barcelona Graduate School of Economics 
}
\end{abstract}

\vspace{1cm}


\vspace{0.5cm}
\end{frontmatter}

\headsep 35pt % So that header does not go over title

\section{Introduction} \label{introduction}
The main goal of this course is to develop an understanding for the reasons why certain machine learning algorithms work. Interestingly, some modern methods, such as Deep Learning, are not fully understood. It is not clear why these methods work or theory even says they shouldn't be as succesful. 


We will start with discussing basic concentration inequalities, followed up by simple mean estimation. After that we'll start discussing supervised learning problems, mostly focused on classification with a minor detour towards regression. We then dive into the topic of empirical risk minimization and VC-theory. This will be followed by a discussion of linear classification, mostly support vector machines and kernel methods. Following, we transition to non-linear methods, especially classification trees and random forests. We finish the course with a discussion of clustering, spectral clustering and k-means and finally online-learning. 

\section{Mean Estimation} \label{mean_estimation}

\subsection{Motivation}

We start the course with a seemingly simple task: estimating the mean of a population, given a sample drawn from the population.

The simplest considerable problem is to consider a setting where we are given independent, identically distributed (i.i.d) draws $X_1, X_2, ... , X_n$ of real-valued random variables. We further assume, that the mean (the expected value) exists $E[X] = m$. (Note that not all distributions have an expected value, such as the cauchy distribution).

Our goal is now, to find an estimate of $m$, based on the observed data. 

An estimator is a function $m_n : \mathbb{R}^n \rightarrow \mathbb{R}$ that maps inputs to a value. We denote our estimate of the mean as the output of the function given the data $m_n(X_1, X_2, ..., X_n) = m_n$ (Note that the value of the estimate is commonly also denoted as $m_n$. 

It is important to realize, that $m_n$ is a function of random variables, so naturally, $m_n$ is also a random variable. Ultimately, we would like to have an estimate (i.e., a data-based quantity) $m_n$ that is close to the real mean $m$. We now need to figure out what "close" means.

\subsection{Measuring the Error}

A possible, and common way to measure the error is through the mean squared error (MSE)

$$ \text{MSE} = E[(m_n - m ) ^2]$$

(Some terminology: The MSE is the risk of the estimator $m_n$ under the squared loss.) However, the MSE is not the only possible measure of "closeness". Others are:

\begin{itemize}
\item Expected absolute error: $E[|m_n -m|]$
\item Using probabilities: $P(|m_n - m| > \epsilon) $
\end{itemize}

We can also discuss the closeness in terms of loss functions $l: \mathbb{R} \rightarrow [0, \infty)$. The corresponding risk is the expected loss $E[l(m_n -m)]$.
The loss functions associated with the discussed errors are:

\begin{itemize}
\item MSE: $l(x) = x^2$
\item Absolute error: $l(x) = |x|$
\item Probability : $l(x) = \mathbbm{1}_{|x|>\epsilon}$
\end{itemize}

These criteria of closeness are not the same! So in order to assess an estimator, we first have to set a goal, in which sense do we want the estimator to be "good". 

\subsection{A simple Estimator}

The most natural mean estimator is the \textbf{empirical mean}.

$$ \overline{m_n} = \frac{1}{n} \sum_{i=1}^{n} X_i $$

By the law of large numberers, as the sample size increases, the probability that the sample mean is equal to the true mean converges to 1. 

Further, the sample mean is an unbiased estimate of the true mean, since:

$$E[m_n] =  E[\frac{1}{n} \sum_{i=1}^{n} X_i] = \frac{1}{n}\sum_{i=1}^{n} E[X_i] = m$$

(By the linearity of expectations). 

We can now derive the MSE of the sample mean. Since we've shown that the estimator is unbiased, the MSE is the variance of the mean estimator. 


\begin{equation*} 
\begin{split}
E[(m_n -m)^2] & = E[(\frac{1}{n} \sum_{i=1}^{n}X_i -m)^2] = \text{var}(m_n) = \text{var}(\frac{1}{n} \sum_{i=1}^{n}X_i) \\ 
&= \frac{1}{n^2} \text{var}(\sum_{i=1}^{n} X_i) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}

(By the linearity of the variance of independent random variables.)
Note that this is only meaningful of the variance is $\sigma^2$ is fininte. Otherwise the MSE is also infinite. 

What does this formula suggest? The error $|m_n - m |$ is typically of the order of $\frac{\sigma}{\sqrt{n}}$. This can be derived from the following observation:

We know from the properties of the variance (not smaller than 0) that:

$$(E[X])^2 \leq E[X^2]$$ 

since $\text{Var}(X) = E[X^2] - (E[X])^2$. From this follows, that $E[X] \leq \sqrt{E[X^2]}$ and thus

$$ E[|m_n -m |] \leq \sqrt{E[m_n - m)^2]} = \frac{\sigma}{\sqrt{n}}$$

Thus, we've established a first bound for the MSE of the sample mean. We can say that the expected distance between the sample mean and the true mean depends on the variance and the sample size.

Very often one needs control probabilities of the type 

$$P(|m_n - m| > \epsilon) $$

This will especially be important when wee need to estimate the mean of not just one but many random variables say $N$ of them and we want to make sure that the errors are simultaneously small. In other words, we need to control 

$$max | m_n^{(j)} - m^{(j)}|$$ 

In order to deal with these types of probabilities and find proper bounds, we'll need to establish some common inequalities.

\subsection{Concentration Inequalities}

Concentration inequalities provide bounds on how a random variable deviates from some value (here often it's expected value). These inequalities can be sorted according to how much information about the random variable is needed in order to use them.

\subsubsection{Markov's Inequality} \label{markov_ineq}

We start with the Markov inequality. The inequality yields an upper bound for the probability that a random variable $X$ exceeds a given value $t$. Using Markov's inequality we don't need any information about the random variable, expect that it's expected value exists.

We are using the special case of the Markov inequality where $X \geq 0$. Then

$$ P(X \geq t) \leq \frac{E[x]}{t}$$

\underline{Proof}

Let $\mathbbm{1}_t$ be the indicator function that event $t$ occurs.  We have $\mathbbm{1}_{(X \geq t)} = 1$ if $X \geq t$ and $\mathbbm{1}_{(X \leq t)} = 0$ otherwise. Then given that $t > 0$ we find

$$ t \mathbbm{1}_{(X \geq t)} \leq X $$

since if $X < t$ then $\mathbbm{1}_{(X \leq t)} = 0$ and so $t \mathbbm{1}_{( X > t ) } = 0 \leq X$. Othweise, if$X \geq t$, we have $\mathbbm{1}_{(X \leq t)} = 1$ and thus $t \mathbbm{1}_{( X > t ) } = t \leq X$. Taking the expecations on both sides:

$$ E[t \mathbbm{1}_{(X \geq t)}]  \leq E[X] $$

using 

$$ t E[\mathbbm{1}_{(X \geq t)}] =  t(  \cdot \mathrm { P } ( X \geq t ) + 0 \cdot \mathrm { P } ( X < t ) ) = t \mathrm { P } ( X \geq t )$$

Thus we have

$$ t P(X \geq t) \leq E[X] $$

\subsubsection{Chebyshev's Inequality} \label{chebyshev}

Chebyshev's inequality bounds the probability that a random variable deviates from it's expected value by more than a given threshold by the variance of the random variable itself. Thus, the use of Chebyshev's inequality requires information about the variance.

$$ P(| X - E[X] | \geq t) \leq \frac{\text{Var}(X)}{t^2}$$

\underline{Proof}

The proof can be derived by using Markov's inequality and by transforming the inequality through taking the square.

$$ P( | X- E[X] | \geq t) = P( (X - E[X] )^2 \geq t^2) \leq \frac{(E[X - E[X])^2}{t^2} = \frac{\text{Var}(X)}{t^2}$$

In particular, for the sample mean we find:

$$ P(| m_n - m | \geq \epsilon) \leq \frac{\text{Var}(m_n)}{t^2} = \frac{\sigma^2}{n\epsilon^2} $$

This implies the weak law of large numbers. The probability that the difference between the sample mean and the true mean is larger than a given $\epsilon$ converges to zero as the sample size grows, for all $\epsilon > 0 $.

\subsubsection{Chernoff Bounds}\label{chernoff}

The Chernoff bound describes exponentially decreasing bounds on tail distributions of sums of indenpendent random variables. It is a sharper bound than Markov's ineqaulity and Chebyshev's inequlaity. 

We use the Chernoff bound since we would like to have sharper bounds for $ P( X - E[X] \geq t) $. 

The Chernoff bound for a random variable $X$ is attained by applying the exponential function:

$$ P( X - E[X] \geq t )  = P( e^{\lambda(X- E[X])} \geq e^{\lambda t)}) \leq \frac{E [\text{exp}(\lambda(X- E[X])]}{\text{exp}(\lambda t)}$$

For any $\lambda > 0 $.  The function $E[e^{tX}]$ of the random variable $X$ is called the \textbf{moment generating function}.  So the probability that $X$ exceeds it's expected value by $t$ is bound by the moment generating function. We can now bound the moment generating function and optimise the bound in $\lambda$. 

\underline{Example} 

Taking a random variable $X \sim N(0,1) $ we want to derive $P(X \geq t)$. The moment generating function is $e^{\lambda^2 / 2}$. We therefore have 

$$ P(X \geq t) \leq \frac{e^{\lambda ^2 / 2}}{e^{\lambda t}} = \text{exp}(\lambda^2 / 2 - \lambda t)$$

Optimizing this w.r.t $\lambda$ we get that $\lambda = t$.


\subsection{Bounds for the Mean Estimator}

Having discussed these bounds, we can now apply them to find bounds for the mean estimator discussed before. We want to get an upper bound for the probability that our mean estimator deviates from the true mean by more than a value $t$. 

Let $X_1, ... , X_n$ be i.i.d random variables with mean $m$. Using Chernoff bounds we find:

$$ P(m_n - m \geq t) = P(\sum_{i=1}^n X_i - mn \geq nt) \leq \frac{E[exp(\lambda\sum_{i=1}^n X_i - m)]}{exp(\lambda nt)}$$

In the numerator, we drop $n \dot m$ since we can pull the $m$ into the sum $n$ times. To further simplify this upper bound for the mean we can simplify the numerator:

\begin{equation*} 
\begin{split}
E[e^{\lambda \sum_{i=1}^{n} (X_i - m) } ] &= E[\prod_{i=1}^n e^{\lambda (X_i -m)}] \\
&= \prod_{i=1}^n E[e^{\lambda (X_i -m)}] = (E[e^{\lambda (X_1 - m)}])^n
\end{split}
\end{equation*}

The first equality is due to the properties of exponents, the second transformation is due to the independence of the $X_i$ and the third is due to the fact that the $X_i$ are identical. 

Using this result we find:

$$ P( m_n - m \geq t ) \leq \frac{(E[e^{\lambda (X_1 - m)}])^n}{e^{\lambda nt}}$$

The probability that the estimated mean deviates from the true mean by $t$ is bound by the moment generating function and the number of observations $n$. 

\subsection{Hoeffding's Lemma}

Hoeffding's lemma is an inequality that bounds the moment generating function of any bounded random variable. 

Let $X$ be any real valued random variable that is bounded in the interval $[0,1]$. Then 

$$ E[e^{\lambda (X- E[X])}] \leq e^{\frac{\lambda^2}{8}} $$

Hoeffding's lemma shows, that any bounded random variable is a subgaussian. In general for any interval $[a,b]$ we get:

$$ E[e^{\lambda (X- E[X])}] \leq e^{\frac{\lambda^2(b-a)^2}{8}} $$

\subsection{Hoeffding's Inequality}

Hoeffding's inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a ceratin amount. 

Let $X_1, ... , X_n$ be indepdent, random variables taking values in $[0,1]$ with $E[X] = m$. Then the moment generating function of the sum of these variables is bounded by Hoeffding's lemma. 

$$E[e^{\lambda \sum_{(i=1)}^n (X_i - m)}] = \prod_{(i=1)}^n E[e^{\lambda (X_i -m) } ] \leq e^{\frac{n \lambda^2}{8}}$$

Going back to the problem of estimating the mean from data we get (by applying Chernoff bounds):

$$P(m_n - m \geq \epsilon ) \leq \frac{e^{n \lambda^2 / 8}}{e^{\lambda n \epsilon}} = \text{exp} (n[\lambda^2/8 - \lambda \epsilon]) $$

We can minimize this bound with respect to $\lambda$. The optimization yields $\lambda = 4 \epsilon$. Plugging this in to the formula we get \textbf{Hoeffding's inequality}. 

$$ P (m_n - m \geq \epsilon) \leq e^{-2n \epsilon^2}$$ 

For this bound to hold, we don't need much, just independence between the random variables and that they are bounded by $[0,1]$. Hoeffding's inequality therefore gives non-asymptotic and distribution free bound for the distance of the mean estimate from the true mean. 

\subsection{Bernstein's Inequality}

Hoeffding's inequality is elegant and easy to use but (because of it's distribution-free nature) it is necessarily not tight for some distributions. In particular, the dependen on the variance is missing from the exponent. Using the Chernoff bound, one may prove such a bound, called Bernstein's inequality. 

Let $X_1, ..., X_n$ be independent random variables such that $X_i \leq 1$, $E[X_i] = 0$ and $\text{Var}(X_i) = \sigma ^2$ (bounded by 1, zero mean and finite variance). Then for some $b > 0 $

$$P ( \sum_{i = 1}^n X_i \geq t) \leq \text{exp}[\frac{-t^2}{2( \sigma + \frac{bt}{3})}]$$ 

By symmetry we can also say that

$$P(m_n - m \leq -\epsilon) = P(m - m_n \geq \epsilon)$$ 

Accordingly, the probability that the absolute error is bigger than $\epsilon$ is 

$$P( | m_n - m | \geq \epsilon) \leq 2 e^{-2t^2 / n}$$

\subsection{The Union Bound}

The union bound, also known as Boole's inequality, says that for any finite or countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events. 

$$\mathbb { P } \left( \bigcup _ { i } A _ { i } \right) \leq \sum _ { i } \mathbb { P } \left( A _ { i } \right)$$


\subsection{The Median of Means Estimator}

Having discussed the various bounds we can now assess the upper bound for the empirical mean. 

Given i.i.d data $X_1, ... , X_n$ with expected value $E[X]] = m$ and finite variance, then using Chebyshev's inequality we find

$$ P( | m_n - m| \geq \epsilon) \leq \frac{\sigma^2}{n \epsilon^2}$$

this is the best assessment we can make, since we can't assume that the data is bounded. The question is now, whether there is a better estimator than the empirical mean. We will now introduce such an estimator, called the median of means estimator.

\subsubsection{Median of Means}

We divide the data $X_1, X_2, ... , X_n$ into $k$ blocks. For simplicity we assume that $n=km$. So block one will be $X_1, ... , X_m$, block two will be $X_{m+1},..., X_{2m}$ and so on up to block $k$.The median of the means estimator computes the empirical mean $\mu_i$  in each block. We then compute the median of these means ($\hat{m_n}$) to obtain the estimate of the mean of the full data. We can now analyse the quality of this estimation. 
For each single estimate $\mu_i$ we know that by Chebyshev's inequality:

$$ P(| \mu_i - m | \geq \frac{2 \sigma }{\sqrt{m}}) \leq \frac{1}{4}$$

\underline{Proof}:

Using Chebyshev's inequality we can write:
$$P(| \mu_i - m | \geq \frac{2 \sigma }{\sqrt{m}}) \leq \frac{\text{Var}(\mu_i)}{\frac{2 \sigma }{\sqrt{m}}} $$

The variance of the mean estimator can be derived using the linearity property of the variance of i.i.d data.

\begin{equation*} 
\begin{split}
\text{Var}(\mu_i) &= \text{Var}(\frac{1}{m} \sum_{i=1}^{m}X_i) \\
&= \frac{1}{m^2} \sum_{i=1}^{m} \text{Var}(X_i) \\
&= \frac{\sigma^2}{m}
\end{split}
\end{equation*}

Using the variance, we arrive at the equation shown above. 


Now the question is, how good is the median of the means estimate? By the property of the median, if $|\hat{m_n} - m | > \frac{2 \sigma }{\sqrt{m}} $ then it must be that at most $k/2$ of the $\hat{\mu_i}$ are such that $|\hat{\mu_i} - m | > \frac{2 \sigma }{\sqrt{m}} $. The probability of this is at most

$$P( \text{B}(K,1/4) \geq k/2) = P(B - k/4 \geq k/4) \leq e^{-2k/16}$$

by Hoeffding's inequality (where B is a binomial random variable). Fixing the allowed eror $\delta$ to be $\delta \in (0,1)$ then $e^{-k/8} \leq \delta$ implies $k\geq 8 \log(1/\delta)$. So we choose the number of blocks to be $k = \ceil{8\log(1/\delta) }$ depending on our allowed erro. Then $ m= \frac{n}{k} = \frac{n}{\ceil{8\log(1/\delta) }}$ and we obtain the following.

\underline{Theorem}:

Fix $\delta \in (0,1)$. The mediean of means estimator with $k \ceil{8\log(1/\delta)}$ blocks satisifies 

$$P(|\hat{m_n} - m | \geq \frac{2\sigma \sqrt{8 \log(1/\delta)}}{\sqrt{n}}) \leq \delta $$

The median of means estimator has sub-Gaussian performance under the only condition that the variance is finite. This is much better than the empirical mean. The estimate depends on $\delta$, for each $\delta$ we have a different estimate. 

\section{Random Projections for Dimensionality Reduction}

We can illustrate the power of concentration inequalities (in particular, Chernoff bounds) by a suprising dimensionality reduction technique that has many applications. 

Let $a_1, ... , a_n \in \mathbb{R}^D$ where D is large (in genetics for example). We would like to find a mapping $f : \mathbb{R}^2 \rightarrow \mathbb{R}^d $ such that $f$ preserves pairwise distances in the sense that for all $i,j = 1,..,n$ 

$$|| f(a_i) - f(a_j)|| \approx || a_i - a_j || $$

In other words, we'd like to represent these points in a lower dimension space, without loss of information.  If exact equality is required, there's not much one can do (unless all points fall in a low-dimensional subspace of $\mathbb{R}^D$). However, if we allow some error, the situation changes dramatically.

Let $\epsilon > 0$. We require that for all $i,j = 1,...,n$ 

$$ 1-\epsilon \leq \frac{||f(a_i) - f(a_j)||^2}{||a_i - a_j||^2} \leq 1 + \epsilon $$ 

In other words, we want the ratio of euclidian distances to be close to one, with error $\epsilon$. Surprisingly, such an $f : \mathbb{R}^2 \rightarrow \mathbb{R}^d $ exists, whenever $d \geq \frac{8\log(n)}{\epsilon^2}$ \underline{independently} of how large $D$ is. This is also known as the Johnson-Lindenstrauss-Lemma and used in compressed sensing, dimensionality reduction and graph embedding. 

It is perhaps suprising how easy it is to find such functions. In fact, we can take $f$ to be linear, that is, $f$ is of the form $f(a) = Wa$ where $W =(W_{ij})_{dxD}$ is a matrix. How do we find such a matrix? We can pick it at \underline{random}! We show that if $W_{ij} \sim N(0, 1/d)$ all of them independent, then the matrix $W$ has the desired property, with high probability. 

We note: $f(a_i) - f(a_j) = W(a_i-a_j) = Wb_{ik}$. For any vector $b \in \mathbb{R}^D$:

\begin{equation*}
\begin{split}
E[||Wb||^2]  &= E[(\sum_{i=1}^d\sum_{j=1}^D b_j W_ij)^2] \\
&= \sum_{i=1}^d E[(\sum_{j=1}^D b_j W_{ij})^2] \\
&= \sum_{i=1}^d E[\sum_{j=1}^D \frac{1}{d}b_j^2] \\
&= \frac{d}{d} \sum_{j=1}^D b_j^2 = ||d||^2
\end{split}
\end{equation*}

because the $W_{ij}$ are independent and have variance $\frac{1}{d}$. 

\noindent\rule[0.5ex]{\linewidth}{1pt}
Short info for the third step in the derivation:
We know that $\sum b_i N_i \sim N(0,\frac{\sum b_i^2}{d})$ if $N_i \sim N(0,1/d) (i.i.d) $. We now want to find $E[(\sum_{j=1}^D b_j W_{ij})^2]$. We can make use of the variance: 

$$ \text{Var}(X) = E(X^2) - (E(X))^2 $$

where $ X\sum_{j=1}^D b_j W_{ij}$. In this case we know that $(E(X))^2 = 0 $, by independence. Therefore, $E(X^2) = \text{Var}(X) = ,\frac{\sum b_i^2}{d}$

\noindent\rule[0.5ex]{\linewidth}{1pt}

In particular, for any $a_i,a_j$

$$E[||f(a_i) - f(a_j)||^2] = E[||(a_i - a_j) ||^2] = ||a_i -a_j||^2$$

So one needs to show that, with high probability 

$$ \max_{i,j = 1,...,n} \left|\frac{||W(a_i-a_j)||^2}{||a_i -a_j||^2} -1\right| < \epsilon$$

We can re-write: 

$$\frac{|| W(a_i -a_j)||^2}{||a_i -a_j||^2} = \left|\left|W\frac{a_i-a_j}{||a_i-a_j||}\right|\right|^2 = ||Wb_{ij}||^2$$

Where $b_{ij}$ is a unit vector. But for any unit vector $b \in \mathbb{R}^D$,

$$ ||Wb||^2 -1 = \sum_{i=1}^d(\sum_{j=1}^D W_{ij}b_j)^2 -1 = \sum_{i=1}^d(N_i^2-E(N_i^2))$$

Where $\sum _ { j = 1 } ^ { D } W _ { i j } c _ { j } = N _ { i }$.

This can be shown by using the fact that are i.i.d $W_{ij} \sim N(0,\frac{1}{d})$. Due to independence $E[\sum_{j=1}^D W_{ij}b_j] =  \sum_{j=1}^D b_j E[W_{ij}] = 0$. The variance is $\text{Var}(\sum_{j=1}^D W_{ij}b_j) = \sum_{j=1}^D b_j^2 \text{Var}(W_{ij}) = \text{Var}(W_{ij}) = \frac{1}{d}$ since $b$ is a unit vector and for any unit vector $b$, $\sum_{j=1}^D b_j^2 = 1$. Furthermore, $\sum_{i=1}^dE[N_i^2] = \sum_{i=1}^d \frac{1}{d}= 1$. 


Note, that $\sum _ { i = 1 } ^ { d } N _ { i } ^ { 2 } \sim \mathcal { X } ^ { 2 } ( d )$ is a sum of squared random normal variables. We may now prove, using the Chernoff bound, that 

$$P (| ||Wb||^2 -1 | > \epsilon) \leq e^{-d \epsilon^3 / 4}$$

and therefore, by the union bound, since there are $\frac{n(n-1)}{2} < n^2$ points $a_i,a_j$

$$P\left( \max_{i,j=1,...,n} \left| \frac{||W(a_i-a_j)||^2}{||a_i-a_j||^2}\right| > \epsilon )\right) \leq n^2 e^{-\epsilon^3 d/4}$$

Whenever $d > \frac{4 \log(n^2/\delta)}{\epsilon^2}$. 

\section{Binary Classification}
\subsection{The Classification Problem}

We are all familiar with the issue of binary classification, say classifying animals into cats and dogs, depending on their weight and tail length. However, in order to assess whether we are doing the right thing when we apply a model, we need a mathematical framework. This section will help to develop an understanding of such a framework. 

Usually, the starting point of (supervised) binary classification is a set of observations, in our case denoted by $\chi$. We want to assign each of these observations $x \in \chi$, to one of the two classes, say $0$ or $1$. Formally, a classifier is a function $g: \chi \rightarrow \{0,1\}$. The problem can be set up as a statistical hypothesis testing problem. In this framework, one assumes that the observation $X$ is random, and it has different (conditional) distributions depending on which of the two classes it belongs to. Formally, $(X,Y)$ is a pair of random variables taking values in $ \chi x \{0,1\}$. $X$ represents the observation and $Y$ its class or label. 

To make the further discussion more concise, we can establish some properties of the point distribution of $(X,Y)$. Most importantly, we want to make statments about the distribution of our parameters or features $X$ which we will call $\mu$ and the a posteriori probabilities of $Y$ given $X$, which we'll call $\eta$. Additionally, we can describe the a priori probabilities of an observation belonging to either class $i$ by $q_i$ and the conditional distribution of $X$ depending on $Y$ as the class conditional distributions. In general we may say that, $X$ can take values in a subset of $\chi$ which we'll call $A$.

\begin{itemize}
\item $\mu (A) = P(X \in A)$ for all $A \subset \chi$
\item $\eta: \chi \rightarrow [0,1]$, $\eta(x) = P(Y=1 | X=x)$.
\item $q_1 = P(Y=1), q_0 = P(Y=0)$
\item $P(X \in A | Y= 0) ,  P(X \in A | Y = 1)$ for $A \subset \chi$
\end{itemize}

\subsection{Bayes Risk and Classifier}

Having set up the problem formally, we now want to make statments about the quality of a classifier or a prediction rule. In order to do so, we first need to find a baseline classifier, to which we can compare other classifiers. Ideally, this classifier has the optimal decision rule! So what is the optimal decision rule? Let's say for now, that we want to minimize the probability of error. Formally we want to  \textbf{minimize the risk} $R(g)$ of classifier $g$, given by its probability of error.

$$R(g) = P(g(x) \neq y)$$

Let's further assume, that we measure errors in a symmetric way. \\
$$R(g) = P(g =1 |Y= 0) P(Y=0) + P(g =0 | Y= 1) P(Y=1) $$  

This may not be natural (or wise) in some applications. For more general cases, we may have a loss function $l$, which measures the different types of errors. 

$$ l: Y \times Y \rightarrow R$$ 

Given a loss function, the risk is the expected value of said function $R(g) = E[l(g(X),Y)]$

Coming back to minimizing the probability of error: Let's say that we know all parameters of the distribution $(X,Y)$, what is the optimal prediction rule which minimizes the probability of error (the risk)? This prediction rule is also called the \textbf{Bayes Predictor} $g^*$ and its risk $R(g^*)$ is the \textbf{Bayes Risk}. We can show, that we minimize the probability of risk, if we classifier observations as belonging to class 1 if we know that their a posteriori probability of belonging to class 1 ($\eta(x)$) is bigger or equal to $\frac{1}{2}$.

\begin{equation*}
g^*(x) = \begin{cases}
1 &\text{if $\eta(x) > 1/2$}\\
0 &\text{otherwise}
\end{cases}
\end{equation*}

We will now show, that this classification rule indeed leads to minimal risk among all classification rules $g$. \\

\begin{theorem}
The Bayes classifier minimizes the probability of error. That is for any classifier $g$, $R(g) \geq R(g^*)$
\end{theorem}

\begin{proof}
First we express the conditional probability of error for any classifier $g$.

\begin{align*}
P\left( g(x) \neq y |X=x \right) &= 1- P\left( g(x)=1,y=1 | X=x\right) - P\left(g(x) = 0, y=0 |X=x \right) \\
&= 1 - \mathbbm{1}_{g(x)=1} P(y=1|X=x) - \mathbbm{1}_{g(x)=0} P(y=0|X=x) \\
&= 1-  \mathbbm{1}_{g(x)=1} \eta(x) -  \mathbbm{1}_{g(x)=0} (1-\eta(x) 
\end{align*}

We can see, that the probability of error depends on the a-posteriori probability $\eta(x)$. Now we compare the Bayes classifier $g^*$ to any other classifier $g$.

\begin{align*}
P(g(x) \neq y |X=x) - P(g^*(x) \neq y | X= x) \\
&= \eta(x) \left( \mathbbm{1}_{g^*(x)=1} - \mathbbm{1}_{g(x)=1} \right) + (1-\eta(x)) \left( \mathbbm{1}_{g^*(x)=0} - \mathbbm{1}_{g(x)=0} \right) \\
&= (2\eta(x) -1 ) (\left( \mathbbm{1}_{g^*(x)=1} - \mathbbm{1}_{g(x)=1} \right) \geq 0 
\end{align*}

Using the fact that $\mathbbm{1}_{g(x) = 0} = 1 -\mathbbm{1}_{g(x) = 1} $. By the defintion of $g^*$, this has to be bigger or equal to zero. Integrating both sides with respect to $\mu(dx)$ yields the theorem. 

\end{proof}

The proof shows, that the loss is

$$L(g) = 1 - \mathbb{E} \left[ \mathbbm{1}_{g(x)=1} \eta(x) \right] -  \mathbb{E} \left[ \mathbbm{1}_{g(x)=0} (1 - \eta(x) )  \right]$$

In particular, the loss of the bayes classifier is

\begin{align*}
L(g^*) &= 1- \mathbb{E} \left[ \mathbbm{1}_{g(x)>\frac{1}{2}} \eta(x) \right] -  \mathbb{E} \left[ \mathbbm{1}_{g(x) \leq \frac{1}{2}} (1 - \eta(x) )  \right] \\
&= \mathbb { E } \left[ \min ( \eta ( X ) , 1 - \eta ( X ) ) \right]
\end{align*}

If $\mathbbm{1}_{g(x)>\frac{1}{2}} $ is true, then the Bayes risk is $1-\eta(x)$, and if $\mathbbm{1}_{g(x)\leq \frac{1}{2}}$ then the Bayes risk is $\eta(x)$.

We can now also make statments about how hard a classification problem is. If the a posteriori probability $\eta(x)$ is close to $\frac{1}{2}$, then the two classes are almost inseperable and the problem is hard. On the other hand if $\eta(x)$ is 1 or 0, the classes are seperable. This doesn't necessarily mean that the problem will be easy to solve, since we generally have no information on $\eta(x)$. Typically, all we have is training data, which is modeled as as indepednednt pairs of random variables $(X_1,Y_1), ... , (X_n,Y_n)$, drawn from the same distribution as $(X,Y)$. The training data set is also written as 

$$D_n( (X_1,Y_1), ... , (X_n,Y_n))$$

given the data, we construct a data-based classifier $g_n(x) = g_n(x,D_n)$. The classifier is random, as it depends on the random data. The probability of error 

$$R(g_n) = P \left(g_n(x) \neq y | D_n \right)$$

is therefore a random variable. Clearly, $R(g_n) \geq R(g^*)$. $R(g_n) - R(g^*)$ is called the excess risk. We would like to construct classifiers, such that $R(g_n)$ is "small". The problem is, that we don't know anything about the distribution - though sometimes we may be willing to assume something about it. Further, what does it mean for a random variable to be small? We could possibly try to minimize $\mathbb{E} \left[ R(g_n)  \right] - R(g^*)$ or try to choose $g$ so that $R(g_n) - R(g^*)$ is small with probability $1-\delta$. We will often relax our goal and fix a class $\mathbb{C}$ of classifiers $g$ and try to minimize the risk over this particular class. 

\section{Nearest Neighbor Classification}

A simple but effective classification rule (in terms of performance) is the nearest neighbor classifier. Let's assume that our set of observations $\chi$ lives in a metric space, so we can measure distances. (With the ususal properties, non-negativity, triangle-inequality, etc.). Possible distance metrics are the euclidean distance, Manhattan distance ($l_1$ norm), the $l_\infty$ norm or even weighted versions of them.

A natural data-based classifier in this setting is the nearest neighbor rule:

$$g_n(x) = Y_{(1)}(x)$$

where $(X_{(1)},Y_{(1)}(x)), \dots , (X_{(1)},Y_{(1)}(x))$ are the data points ordered according to increasing values of $d(x,X_i)$. Thus, $Y_1(x)$ is the label of the nearest neighbor of $x$. What's appealing about this, is that it's a non-parametric classifier.

The underlying principle of the nearest neighbor rule is that observations which are close to each other are similar. So they will either belong to the same class, or they might even 


\end{document}
