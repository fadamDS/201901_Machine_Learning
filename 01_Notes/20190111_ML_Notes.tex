\documentclass[12pt, authoryear]{elsarticle}
\makeatletter
\def\ps@pprintTitle{%
	\let\@oddhead\@empty
	\let\@evenhead\@empty
	\def\@oddfoot{}%
	\let\@evenfoot\@oddfoot}
\makeatother
%\usepackage{lmodern}
% My spacing
\usepackage{setspace}
\setstretch{1.5}

%\DeclareMathSizes{12}{14}{10}{10}
\usepackage[margin=2.5cm]{geometry}    % How to set margins - optimized for 2.5cm      

% See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   			% ... or a4paper or a5paper or ... 
\usepackage{enumitem}
\usepackage{mathtools}
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    			% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}						% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{flafter}			
\usepackage{setspace}
%\linespread{1.5}
\usepackage[font={}]{caption}
\usepackage[bottom]{footmisc}
\usepackage[capposition=top]{floatrow}   %figure notes

%math packages 
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{graphicx,epsf,subfigure}
\usepackage{pstricks,pst-node,psfrag}
\usepackage{amsthm,amssymb,amsmath}
\usepackage{amsmath,bm}
\usepackage{amsmath}

%mathnotes
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bX}{\mbox{\boldmath $X$}}
\newcommand{\bY}{\mbox{\boldmath $Y$}}
\newcommand{\bI}{\mbox{\boldmath $I$}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\x}{\textsc{\textbf{x}}}
\newcommand{\xx}{\textsc{x}}

%add figure 
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{hyperref}
\usepackage[round]{natbib}

\usepackage{soul}

\def\bibsection{\section{References}} %%% Make "References" appear before bibliography
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{tablefootnote}
\usepackage{lscape} 
\usepackage{animate}
\usepackage{bbm}

\renewcommand{\contentsname}{Table of Contents} % change name from Contents to Table of Contents

\usepackage{titlesec}

\setcounter{secnumdepth}{4}

%_______________________________________________________________________________________________________%
%_______________________________________________________________________________________________________%
%\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
%\usepackage{graphicx,multirow}
\usepackage{xcolor,colortbl}
\usepackage{xcolor}
%\usepackage{graphicx,multirow}
\usepackage[capposition=top]{floatrow}
\setcounter{secnumdepth}{4}

\begin{document}

\begin{frontmatter}  %

\title{Lecture Notes Machine Learning}

\author[Add1]{Felix Adam}
\ead{felix.adam@bracelonagse.eu}

\address[Add1]{Barcelona Graduate School of Economics, Barcelona, Spain}
%\address[Add2]{Some other Institution, Cape Town, South Africa}

%\cortext[cor]{Corresponding author: Nico Katzke}

\begin{abstract}
\small{
This document contains my lecture notes for the course Machine Learning at the Barcelona Graduate School of Economics 
}
\end{abstract}

\vspace{1cm}


\vspace{0.5cm}
\end{frontmatter}

\headsep 35pt % So that header does not go over title

\section{Introduction} \label{introduction}
The main goal of this course is to develop an understanding for the reasons why certain machine learning algorithms work. Interestingly, some modern methods, such as Deep Learning, are not fully understood. It is not clear why these methods work or theory even says they shouldn't be as succesful. 


We will start with discussing basic concentration inequalities, followed up by simple mean estimation. After that we'll start discussing supervised learning problems, mostly focused on classification with a minor detour towards regression. We then dive into the topic of empirical risk minimization and VC-theory. This will be followed by a discussion of linear classification, mostly support vector machines and kernel methods. Following, we transition to non-linear methods, especially classification trees and random forests. We finish the course with a discussion of clustering, spectral clustering and k-means and finally online-learning. 

\section{Mean Estimation} \label{mean_estimation}

\subsection{Motivation}

We start the course with a seemingly simple task: estimating the mean of a population, given a sample drawn from the population.

The simplest considerable problem is to consider a setting where we are given independent, identically distributed (i.i.d) draws $X_1, X_2, ... , X_n$ of real-valued random variables. We further assume, that the mean (the expected value) exists $E[X] = m$. (Note that not all distributions have an expected value, such as the cauchy distribution).

Our goal is now, to find an estimate of $m$, based on the observed data. 

An estimator is a function $m_n : \mathbb{R}^n \rightarrow \mathbb{R}$ that maps inputs to a value. We denote our estimate of the mean as the output of the function given the data $m_n(X_1, X_2, ..., X_n) = m_n$ (Note that the value of the estimate is commonly also denoted as $m_n$. 

It is important to realize, that $m_n$ is a function of random variables, so naturally, $m_n$ is also a random variable. Ultimately, we would like to have an estimate (i.e., a data-based quantity) $m_n$ that is close to the real mean $m$. We now need to figure out what "close" means.

\subsection{Measuring the Error}

A possible, and common way to measure the error is through the mean squared error (MSE)

$$ \text{MSE} = E[(m_n - m ) ^2]$$

(Some terminology: The MSE is the risk of the estimator $m_n$ under the squared loss.) However, the MSE is not the only possible measure of "closeness". Others are:

\begin{itemize}
\item Expected absolute error: $E[|m_n -m|]$
\item Using probabilities: $P(|m_n - m| > \epsilon) $
\end{itemize}

We can also discuss the closeness in terms of loss functions $l: \mathbb{R} \rightarrow [0, \infty)$. The corresponding risk is the expected loss $E[l(m_n -m)]$.
The loss functions associated with the discussed errors are:

\begin{itemize}
\item MSE: $l(x) = x^2$
\item Absolute error: $l(x) = |x|$
\item Probability : $l(x) = \mathbbm{1}_{|x|>\epsilon}$
\end{itemize}

These criteria of closeness are not the same! So in order to assess an estimator, we first have to set a goal, in which sense do we want the estimator to be "good". 

\subsection{A simple Estimator}

The most natural mean estimator is the \textbf{empirical mean}.

$$ \overline{m_n} = \frac{1}{n} \sum_{i=1}^{n} X_i $$

By the law of large numberers, as the sample size increases, the probability that the sample mean is equal to the true mean converges to 1. 

Further, the sample mean is an unbiased estimate of the true mean, since:

$$E[m_n] =  E[\frac{1}{n} \sum_{i=1}^{n} X_i] = \frac{1}{n}\sum_{i=1}^{n} E[X_i] = m$$

(By the linearity of expectations). 

We can now derive the MSE of the sample mean. Since we've shown that the estimator is unbiased, the MSE is the variance of the mean estimator. 


\begin{equation*} 
\begin{split}
E[(m_n -m)^2] & = E[(\frac{1}{n} \sum_{i=1}^{n}X_i -m)^2] = \text{var}(m_n) = \text{var}(\frac{1}{n} \sum_{i=1}^{n}X_i) \\ 
&= \frac{1}{n^2} \text{var}(\sum_{i=1}^{n} X_i) = \frac{\sigma^2}{n}
\end{split}
\end{equation*}

(By the linearity of the variance of independent random variables.)
Note that this is only meaningful of the variance is $\sigma^2$ is fininte. Otherwise the MSE is also infinite. 

What does this formula suggest? The error $|m_n - m |$ is typically of the order of $\frac{\sigma}{\sqrt{n}}$. This can be derived from the following observation:

We know from the properties of the variance (not smaller than 0) that:

$$(E[X])^2 \leq E[X^2]$$ 

since $\text{Var}(X) = E[X^2] - (E[X])^2$. From this follows, that $E[X] \leq \sqrt{E[X^2]}$ and thus

$$ E[|m_n -m |] \leq \sqrt{E[m_n - m)^2]} = \frac{\sigma}{\sqrt{n}}$$

Thus, we've established a first bound for the MSE of the sample mean. We can say that the expected distance between the sample mean and the true mean depends on the variance and the sample size.

Very often one needs control probabilities of the type 

$$P(|m_n - m| > \epsilon) $$

This will especially be important when wee need to estimate the mean of not just one but many random variables say $N$ of them and we want to make sure that the errors are simultaneously small. In other words, we need to control 

$$max | m_n^{(j)} - m^{(j)}|$$ 

In order to deal with these types of probabilities and find proper bounds, we'll need to establish some common inequalities.

\subsection{Concentration Inequalities}

Concentration inequalities provide bounds on how a random variable deviates from some value (here often it's expected value). These inequalities can be sorted according to how much information about the random variable is needed in order to use them.

\subsubsection{Markov's Inequality} \label{markov_ineq}

We start with the Markov inequality. The inequality yields an upper bound for the probability that a random variable $X$ exceeds a given value $t$. Using Markov's inequality we don't need any information about the random variable, expect that it's expected value exists.

We are using the special case of the Markov inequality where $X \geq 0$. Then

$$ P(X \geq t) \leq \frac{E[x]}{t}$$

\underline{Proof}

Let $\mathbbm{1}_t$ be the indicator function that event $t$ occurs.  We have $\mathbbm{1}_{(X \geq t)} = 1$ if $X \geq t$ and $\mathbbm{1}_{(X \leq t)} = 0$ otherwise. Then given that $t > 0$ we find

$$ t \mathbbm{1}_{(X \geq t)} \leq X $$

since if $X < t$ then $\mathbbm{1}_{(X \leq t)} = 0$ and so $t \mathbbm{1}_{( X > t ) } = 0 \leq X$. Othweise, if$X \geq t$, we have $\mathbbm{1}_{(X \leq t)} = 1$ and thus $t \mathbbm{1}_{( X > t ) } = t \leq X$. Taking the expecations on both sides:

$$ E[t \mathbbm{1}_{(X \geq t)}]  \leq E[X] $$

using 

$$ t E[\mathbbm{1}_{(X \geq t)}] =  t(  \cdot \mathrm { P } ( X \geq t ) + 0 \cdot \mathrm { P } ( X < t ) ) = t \mathrm { P } ( X \geq t )$$

Thus we have

$$ t P(X \geq t) \leq E[X] $$

\subsubsection{Chebyshev's Inequality} \label{chebyshev}

Chebyshev's inequality bounds the probability that a random variable deviates from it's expected value by more than a given threshold by the variance of the random variable itself. Thus, the use of Chebyshev's inequality requires information about the variance.

$$ P(| X - E[X] | \geq t) \leq \frac{\text{Var}(X)}{t^2}$$

\underline{Proof}

The proof can be derived by using Markov's inequality and by transforming the inequality through taking the square.

$$ P( | X- E[X] | \geq t) = P( (X - E[X] )^2 \geq t^2) \leq \frac{(E[X - E[X])^2}{t^2} = \frac{\text{Var}(X)}{t^2}$$

In particular, for the sample mean we find:

$$ P(| m_n - m | \geq \epsilon) \leq \frac{\text{Var}(m_n)}{t^2} = \frac{\sigma^2}{n\epsilon^2} $$

This implies the weak law of large numbers. The probability that the difference between the sample mean and the true mean is larger than a given $\epsilon$ converges to zero as the sample size grows, for all $\epsilon > 0 $.

\subsubsection{Chernoff Bounds}\label{chernoff}

The Chernoff bound describes exponentially decreasing bounds on tail distributions of sums of indenpendent random variables. It is a sharper bound than Markov's ineqaulity and Chebyshev's inequlaity. 

We use the Chernoff bound since we would like to have sharper bounds for $ P( X - E[X] \geq t) $. 

The Chernoff bound for a random variable $X$ is attained by applying the exponential function:

$$ P( X - E[X] \geq t )  = P( e^{\lambda(X- E[X])} \geq e^{\lambda t)}) \leq \frac{E [\text{exp}(\lambda(X- E[X])]}{\text{exp}(\lambda t)}$$

For any $\lambda > 0 $.  The function $E[e^{tX}]$ of the random variable $X$ is called the \textbf{moment generating function}.  So the probability that $X$ exceeds it's expected value by $t$ is bound by the moment generating function. We can now bound the moment generating function and optimise the bound in $\lambda$. 

\underline{Example} 

Taking a random variable $X \sim N(0,1) $ we want to derive $P(X \geq t)$. The moment generating function is $e^{\lambda^2 / 2}$. We therefore have 

$$ P(X \geq t) \leq \frac{e^{\lambda ^2 / 2}}{e^{\lambda t}} = \text{exp}(\lambda^2 / 2 - \lambda t)$$

Optimizing this w.r.t $\lambda$ we get that $\lambda = t$.


\subsection{Bounds for the Mean Estimator}

Having discussed these bounds, we can now apply them to find bounds for the mean estimator discussed before. We want to get an upper bound for the probability that our mean estimator deviates from the true mean by more than a value $t$. 

Let $X_1, ... , X_n$ be i.i.d random variables with mean $m$. Using Chernoff bounds we find:

$$ P(m_n - m \geq t) = P(\sum_{i=1}^n X_i - mn \geq nt) \leq \frac{E[exp(\lambda\sum_{i=1}^n X_i - m)]}{exp(\lambda nt)}$$

In the numerator, we drop $n \dot m$ since we can pull the $m$ into the sum $n$ times. To further simplify this upper bound for the mean we can simplify the numerator:

\begin{equation*} 
\begin{split}
E[e^{\lambda \sum_{i=1}^{n} (X_i - m) } ] &= E[\prod_{i=1}^n e^{\lambda (X_i -m)}] \\
&= \prod_{i=1}^n E[e^{\lambda (X_i -m)}] = (E[e^{\lambda (X_1 - m)}])^n
\end{split}
\end{equation*}

The first equality is due to the properties of exponents, the second transformation is due to the independence of the $X_i$ and the third is due to the fact that the $X_i$ are identical. 

Using this result we find:

$$ P( m_n - m \geq t ) \leq \frac{(E[e^{\lambda (X_1 - m)}])^n}{e^{\lambda nt}}$$

The probability that the estimated mean deviates from the true mean by $t$ is bound by the moment generating function and the number of observations $n$. 

\subsection{Hoeffding's Lemma}

Hoeffding's lemma is an inequality that bounds the moment generating function of any bounded random variable. 

Let $X$ be any real valued random variable that is bounded in the interval $[0,1]$. Then 

$$ E[e^{\lambda (X- E[X])}] \leq e^{\frac{\lambda^2}{8}} $$

Hoeffding's lemma shows, that any bounded random variable is a subgaussian. In general for any interval $[a,b]$ we get:

$$ E[e^{\lambda (X- E[X])}] \leq e^{\frac{\lambda^2(b-a)^2}{8}} $$

\subsection{Hoeffding's Inequality}

Hoeffding's inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a ceratin amount. 

Let $X_1, ... , X_n$ be indepdent, random variables taking values in $[0,1]$ with $E[X] = m$. Then the moment generating function of the sum of these variables is bounded by Hoeffding's lemma. 

$$E[e^{\lambda \sum_{(i=1)}^n (X_i - m)}] = \prod_{(i=1)}^n E[e^{\lambda (X_i -m) } ] \leq e^{\frac{n \lambda^2}{8}}$$

Going back to the problem of estimating the mean from data we get (by applying Chernoff bounds):

$$P(m_n - m \geq \epsilon ) \leq \frac{e^{n \lambda^2 / 8}}{e^{\lambda n \epsilon}} = \text{exp} (n[\lambda^2/8 - \lambda \epsilon]) $$

We can minimize this bound with respect to $\lambda$. The optimization yields $\lambda = 4 \epsilon$. Plugging this in to the formula we get \textbf{Hoeffding's inequality}. 

$$ P (m_n - m \geq \epsilon) \leq e^{-2n \epsilon^2}$$ 

For this bound to hold, we don't need much, just independence between the random variables and that they are bounded by $[0,1]$. Hoeffding's inequality therefore gives non-asymptotic and distribution free bound for the distance of the mean estimate from the true mean. 

\subsection{Bernstein's Inequality}

Hoeffding's inequality is elegant and easy to use but (because of it's distribution-free nature) it is necessarily not tight for some distributions. In particular, the dependen on the variance is missing from the exponent. Using the Chernoff bound, one may prove such a bound, called Bernstein's inequality. 

Let $X_1, ..., X_n$ be independent random variables such that $X_i \leq 1$, $E[X_i] = 0$ and $\text{Var}(X_i) = \sigma ^2$ (bounded by 1, zero mean and finite variance). Then for some $b > 0 $

$$P ( \sum_{i = 1}^n X_i \geq t) \leq \text{exp}[\frac{-t^2}{2( \sigma + \frac{bt}{3})}]$$ 

By symmetry we can also say that

$$P(m_n - m \leq -\epsilon) = P(m - m_n \geq \epsilon)$$ 

Accordingly, the probability that the absolute error is bigger than $\epsilon$ is 

$$P( | m_n - m | \geq \epsilon) \leq 2 e^{-2t^2 / n}$$

\subsection{The Union Bound}

The union bound, also known as Boole's inequality, says that for any finite or countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events. 

$$\mathbb { P } \left( \bigcup _ { i } A _ { i } \right) \leq \sum _ { i } \mathbb { P } \left( A _ { i } \right)$$


\subsection{The Median of Means Estimator}

Having discussed the various bounds we can now assess the upper bound for the empirical mean. 

Given i.i.d data $X_1, ... , X_n$ with expected value $E[X]] = m$ and finite variance, then using Chebyshev's inequality we find

$$ P( | m_n - m| \geq \epsilon) \leq \frac{\sigma^2}{n \epsilon^2}$$

this is the best assessment we can make, since we can't assume that the data is bounded. The question is now, whether there is a better estimator than the empirical mean. We will now introduce such an estimator, called the median of means estimator.

\subsubsection{Median of Means}

We divide the data $X_1, X_2, ... , X_n$ into $k$ blocks. For simplicity we assume that $n=km$. So block one will be $X_1, ... , X_m$, block two will be $X_{m+1},..., X_{2m}$ and so on up to block $k$.The median of the means estimator computes the empirical mean $\mu_i$  in each block. We then compute the median of these medians to obtain the estimate of the mean of the full data. We can now analyse the quality of this estimation. 
For each single estimate $\mu_i$ we know that by Chebyshev's inequality (dropping 

$$ P(| \mu_i - m | \geq \frac{2 \sigma }{\sqrt{m}}) \leq \frac{1}{4}$$






\end{document}
